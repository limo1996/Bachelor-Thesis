using System;
using System.Collections.Generic;
using System.Linq;
using System.Runtime.Serialization;
using System.Threading.Tasks;
using Windows.Media.SpeechRecognition;
using Windows.Media.SpeechSynthesis;
using Windows.Storage;
using Windows.UI.Xaml.Controls;
using PiStudio.Win10.Voice.Commands;
using PiStudio.Win10.Voice.Srgs;
using System.Threading;
using Windows.UI;

namespace PiStudio.Win10.Voice.Navigation
{
    /// <summary>
    /// Class that recognizes speech commands and performs action when was command recognized.
    /// </summary>
    public class SpeechNavigator
	{
		private SpeechRecognizer m_constrainedRecognizer;
		private SpeechRecognizer m_freeSpeechRecognizer;
        private VoiceCommands m_voiceCommands;
        private SemaphoreSlim m_semaphore = new SemaphoreSlim(1, 1);
        private Dictionary<string, HashSet<StorageFile>> m_compiledFiles = new Dictionary<string, HashSet<StorageFile>>();
        private Windows.Globalization.Language m_currentLanguage = new Windows.Globalization.Language("en-US");
        private const string m_commandsFilenameSuffix = ".grxml";
        private string m_location = null; 
		private string m_filePath;
        private bool m_constrained = false;
		public static readonly uint HResultPrivacyStatementDeclined = 0x80045509;

		/// <summary>
		/// Creates new instance of <see cref="SpeechNavigator"/>. Language of recognition is English. 
		/// </summary>
		/// <param name="filepath">Path to the folder where are voice commands, and where will be compiled GRXML file stored.</param>
		private SpeechNavigator(string filepath)
		{
			m_constrainedRecognizer = new SpeechRecognizer(new Windows.Globalization.Language("en-US"));
			m_freeSpeechRecognizer = new SpeechRecognizer(new Windows.Globalization.Language("en-US"));
			m_location = filepath;
            SetHypGeneratedEvent();
		}

		/// <summary>
		/// Creates new instance of <see cref="SpeechNavigator"/>.
		/// </summary>
		/// <param name="filepath">Path to the folder where are voice commands, and where will be compiled GRXML file stored.</param>
		/// <param name="language"><see cref="Windows.Globalization.Language"/> in which will be recognizer recognizing.</param>
		private SpeechNavigator(string filepath, Windows.Globalization.Language language)
		{
			m_constrainedRecognizer = new SpeechRecognizer(language);
            m_freeSpeechRecognizer = new SpeechRecognizer(language);
			m_location = filepath;
            m_currentLanguage = language;
            SetHypGeneratedEvent();
		}

        /// <summary>
        /// Gets voice commands that are currently set in speech recognizer.
        /// </summary>
        public VoiceCommands VoiceCommands
        {
            get
            {
                return m_voiceCommands;
            }
        }

        public Windows.Globalization.Language CurrentLanguage
        {
            get
            {
                return m_currentLanguage;
            }
        }

        public async Task SetLanguageAsync(Windows.Globalization.Language language)
        {
            m_constrainedRecognizer = new SpeechRecognizer(language);
            m_currentLanguage = language;
            SetHypGeneratedEvent();

            await m_constrainedRecognizer.CompileConstraintsAsync();
            var vcdFile = await StorageFile.GetFileFromPathAsync(m_filePath);
            await CompileConstraintAsync(vcdFile);
        }
        

        private void SetHypGeneratedEvent()
        {
            var dispatcher = Windows.UI.Core.CoreWindow.GetForCurrentThread().Dispatcher;
            m_constrainedRecognizer.HypothesisGenerated += async (o, e) =>
            {
                if (m_voiceUI != null)
                   await dispatcher.RunAsync(Windows.UI.Core.CoreDispatcherPriority.Normal, () => m_voiceUI.Text = e.Hypothesis.Text);
            };
        }

		/// <summary>
		/// Initializes <see cref="SpeechNavigator"/> so it is ready to listen for user input.
		/// </summary>
		/// <returns><see cref="SpeechRecognitionCompilationResult"/></returns>
		private async Task Initialize()
		{
		    await m_freeSpeechRecognizer.CompileConstraintsAsync();
			await m_constrainedRecognizer.CompileConstraintsAsync();
		}

		/// <summary>
		/// Initializes <see cref="SpeechNavigator"/> so it is ready to listen for user input. Language of recognition is English. 
        /// All voice commands that are not in English will not be installed into recognizer.
		/// </summary>
		/// <param name="file">File with VoiceCommands that will constrain recognizer to specific commands.</param>
		/// <returns><see cref="SpeechRecognitionCompilationResult"/></returns>
		public async Task Initialize(StorageFile file)
		{
			var result = await m_freeSpeechRecognizer.CompileConstraintsAsync();
			await m_constrainedRecognizer.CompileConstraintsAsync();
			await CompileConstraintAsync(file);
		}

		/// <summary>
		/// Asynchronously creates new instance of <see cref="SpeechNavigator"/>.
		/// </summary>
		/// <returns>Reference to new instance of <see cref="SpeechNavigator"/></returns>
		public static async Task<SpeechNavigator> Create()
		{
			var navigator = new SpeechNavigator(ApplicationData.Current.LocalFolder.Path);
			await navigator.Initialize();
			return navigator;
		}

		/// <summary>
		/// Asynchronously creates new instance of <see cref="SpeechNavigator"/>.
		/// </summary>
		/// <param name="file"><see cref="StorageFile"/> where are VoiceCommands located.</param>
		/// <param name="language"><see cref="Windows.Globalization.Language"/> in which will user speak.</param>
		/// <returns>Reference to new instance of <see cref="SpeechNavigator"/></returns>
		public static async Task<SpeechNavigator> Create(StorageFile file, Windows.Globalization.Language language)
		{
			string path = (await file.GetParentAsync()).Path;
			var navigator = new SpeechNavigator(path, language);
			await navigator.Initialize(file);
			return navigator;
		}

		/// <summary>
		/// Gives recognizer commands and improves its recognition capabilities.
		/// </summary>
		/// <param name="file">File with commands that will constrain recognizer.</param>
		/// <returns><see cref="SpeechRecognitionCompilationResult"/></returns>
		public async Task CompileConstraintAsync(StorageFile file)
		{
			m_filePath = file.Path;
			m_location = (await file.GetParentAsync()).Path;
			var commands = await SpeechRecognitionManager.LoadCommandsFromFileAsync(file);
			if (commands == null)
				throw new SerializationException("File content can not be deserialized! File content is probably in bad format..");
			await CompileConstraintAsync(commands);
		}

		/// <summary>
		/// Gives recognizer commands and improves its recognition capabilities.
		/// </summary>
		/// <param name="commands">Commands that will constrain recognizer if their language matches current language.</param>
		/// <returns><see cref="SpeechRecognitionCompilationResult"/></returns>
		public async Task CompileConstraintAsync(VoiceCommands commands)
		{
			this.m_voiceCommands = commands;
			foreach (var commandSet in commands.CommandSets)
			{
				Grammar grammar = SpeechRecognitionManager.CompileGrammar(commandSet, true);
				var result = await CompileConstraintAsync(grammar, commandSet.Name);
                if (result.Status != SpeechRecognitionResultStatus.Success)
                    throw new GrammarCompilationException();
                else
                    m_constrained = true;
			}
		}

		/// <summary>
		/// Saves given commands into file. Creates GrammarFileConstraint and adds it to speech recognition.
		/// </summary>
		private async Task<SpeechRecognitionCompilationResult> CompileConstraintAsync(Grammar commands, string commandsName)
		{
			StorageFolder folder = ApplicationData.Current.LocalFolder;
			StorageFile file = await folder.CreateFileAsync(commandsName + m_commandsFilenameSuffix, CreationCollisionOption.OpenIfExists);
            if((file.DateCreated - DateTime.Now).Milliseconds < 1000)
			    SpeechRecognitionManager.SaveGrammarToFile(file, commands);

            if (string.Compare(m_currentLanguage.LanguageTag, commands.Language, true) == 0)
            {
                var grammarFileConstraint = new SpeechRecognitionGrammarFileConstraint(file, commandsName);
                grammarFileConstraint.IsEnabled = true;
                AddCompiledFile(file, commands.Language.ToLower());

                this.m_constrainedRecognizer.Constraints.Add(grammarFileConstraint);
            }

			return await this.m_constrainedRecognizer.CompileConstraintsAsync();
		}

        private void AddCompiledFile(StorageFile file, string lang)
        {
            lang = lang.ToLower();
            if (!m_compiledFiles.ContainsKey(lang))
                m_compiledFiles.Add(lang, new HashSet<StorageFile>(new SetComparer()));
            m_compiledFiles[lang].Add(file);
        }

        private class SetComparer : IEqualityComparer<StorageFile>
        {
            public bool Equals(StorageFile x, StorageFile y)
            {
                return x.Path == y.Path;
            }

            public int GetHashCode(StorageFile obj)
            {
                return obj.GetHashCode();
            }
        }


        /// <summary>
        /// Recognizes speech constrained by SRGS grammar created from VoiceCommands that were passes in file. 
        /// </summary>
        /// <returns>Returns <see cref="SpeechRecognitionResult"/></returns>
        public async Task<SpeechRecognitionResult> RecognizeSpeechAsync()
		{
            await m_semaphore.WaitAsync();
            var result = await m_constrainedRecognizer.RecognizeAsync();
            m_semaphore.Release();
			
            if (m_voiceUI != null)
                m_voiceUI.Stop();
            if (m_constrained)
			{
				if (result.RulePath == null || result.Confidence == SpeechRecognitionConfidence.Rejected || result.RulePath.Count == 0)
				{
                    //nothing said usually
					return null;
				}
				var tmp = result.RulePath[0];
				if (!string.IsNullOrEmpty(tmp))
				{
					return new SpeechRecognitionResult(result, m_voiceCommands);
				}
				else
					return null;
			}
			else
				return new SpeechRecognitionResult(result);
		}

        private UI.VoiceUI m_voiceUI;
        public async Task<SpeechRecognitionResult> RecognizeSpeechWithUIAsync(Grid displayGrid, Color color,
            int row = 0, int column = 0, int rowSpan = 1, int columnSpan = 1)
        {
            m_voiceUI = new UI.VoiceUI();
            m_voiceUI.Fill = color;
            m_voiceUI.Text = "Listening...";
            Canvas.SetZIndex(m_voiceUI, 1000);
            Grid.SetColumn(m_voiceUI, column);
            Grid.SetRow(m_voiceUI, row);
            if(columnSpan > 0)
                Grid.SetColumnSpan(m_voiceUI, columnSpan);
            if(rowSpan > 0)
                Grid.SetRowSpan(m_voiceUI, rowSpan);
            displayGrid.Children.Add(m_voiceUI);
            m_voiceUI.Start();
            await System.Threading.Tasks.Task.Delay(1200);
            SayText("Listening");
            var result = await RecognizeSpeechAsync();
            displayGrid.Children.Remove(m_voiceUI);
            return result;
        }

        /// <summary>
        /// Recognizes free non constrained speech. 
        /// </summary>
        /// <returns><see cref="SpeechRecognitionResult"/></returns>
        public async Task<SpeechRecognitionResult> RecognizeFreeSpeechAsync()
		{
			var result = await this.m_freeSpeechRecognizer.RecognizeAsync();
			return new SpeechRecognitionResult(result);
		}

		/// <summary>
		/// Performs action according to given result. Actions must be first set.
		/// </summary>
		/// <param name="result">Result of speech recognition.</param>
		/// <returns></returns>
		private void PerformAction(SpeechRecognitionResult result)
		{
			if (result == null)
				throw new ArgumentNullException();
			var cmd = result.RecognizedCommand;
			var recognizedValues = result.ReconizedPhraseListsValues;
			if (cmd == null)
				return;
			string textToSpeak = null;
			if (cmd.Feedback != null)
				textToSpeak = cmd.Feedback.Content;
			if (string.IsNullOrWhiteSpace(textToSpeak))
				textToSpeak = null;
			var customAction = cmd.VoiceAction as CustomAction;
			var dialogAction = cmd.VoiceAction as ShowDialog;
			foreach (var key in recognizedValues.Keys)
			{
				if (textToSpeak != null)
					textToSpeak = textToSpeak.Replace("{" + key + "}", recognizedValues[key]);
				if (customAction != null)
				{
					customAction.ActionContent = customAction.ActionContent.Replace("{" + key + "}", recognizedValues[key]);
					customAction.ActionParameter = customAction.ActionParameter.Replace("{" + key + "}", recognizedValues[key]);
				}
				if (dialogAction != null)
					dialogAction.TextToDisplay = dialogAction.TextToDisplay.Replace("{" + key + "}", recognizedValues[key]);
			}

			//if(result.RecognizedCommand.Feedback != null)
			//	result.RecognizedCommand.Feedback.Content = textToSpeak;
			if (customAction != null)
				result.RecognizedCommand.VoiceAction = customAction;

			if (textToSpeak != null)
				SayText(textToSpeak);
			if (this.CommandRecognized == null)
				cmd.InvokeAction(this, result);
			else
			{
				var fn = this.CommandRecognized;
				fn(this, result);
			}
		}

		/// <summary>
		/// Starts listening to what user says. As soon as will be some command recognized it will trigger action set to this command.
		/// Does nothing if action was not set.
		/// </summary>
		/// <returns></returns>
		public async Task RecognizeAndPerformActionAsync()
		{
			var result = await RecognizeSpeechAsync();
			if (result != null)
				PerformAction(result);
		}

        public async Task RecognizeAndPerformActionWithUIAsync(Grid displayGrid, Color fill,
            int row = 0, int column = 0, int rowSpan = 0, int columnSpan = 0)
        {
            var result = await RecognizeSpeechWithUIAsync(displayGrid, fill, row, column, rowSpan, columnSpan);
            if (result != null)
                PerformAction(result);
        }

		/// <summary>
		/// Speaks given text asynchronously on the background. Can be awaited until speaking of the text finishes.
		/// </summary>
		/// <param name="textToSpeech">Text to be spoken</param>
		/// <returns><see cref="Task"/></returns>
		public static async Task SayTextAsync(string textToSpeech, Grid form)
		{
			if (string.IsNullOrWhiteSpace(textToSpeech))
				return;
			using (var synthesizer = new SpeechSynthesizer())
			{
				MediaElement audioPlayer = new MediaElement();
				var taskSource = new TaskCompletionSource<bool>();
				synthesizer.Voice = SpeechSynthesizer.AllVoices.First(
							i => (i.Gender == VoiceGender.Female));
				SpeechSynthesisStream ttsStream = await synthesizer.SynthesizeTextToStreamAsync(textToSpeech);

				form.Children.Add(audioPlayer);

				audioPlayer.MediaEnded += (o, e) => { taskSource.TrySetResult(true); };
				audioPlayer.PartialMediaFailureDetected += (o, e) => { taskSource.TrySetResult(true); };
				audioPlayer.MediaFailed += (o, e) => { taskSource.TrySetResult(true); };

				audioPlayer.SetSource(ttsStream, "");
				audioPlayer.Play();

				await taskSource.Task;
				((Grid)form).Children.Remove(audioPlayer);
			}
		}

		/// <summary>
		/// Speaks given text asynchronously on the background and can not be awaited.
		/// </summary>
		/// <param name="textToSpeech">Text to be spoken</param>
		public static async void SayText(string textToSpeech)
		{
			if (string.IsNullOrWhiteSpace(textToSpeech))
				return;
			using (var synthesizer = new SpeechSynthesizer())
			{
				MediaElement audioPlayer = new MediaElement();
				synthesizer.Voice = SpeechSynthesizer.AllVoices.First(
							i => (i.Gender == VoiceGender.Female));
				SpeechSynthesisStream ttsStream = await synthesizer.SynthesizeTextToStreamAsync(textToSpeech);

				audioPlayer.SetSource(ttsStream, "");
				audioPlayer.Play();
			}
		}

		/// <summary>
		/// Sets values to phrase list dynamically in runtime.
		/// </summary>
		/// <param name="commandSetName">CommandSet inside VCD voice commands.</param>
		/// <param name="phraseListName">Name of the phraseList inside CommandSet</param>
		/// <param name="phraseListValues">Values to be inserted into phraseList</param>
		/// <param name="clearOldValues">Indicated whether clear old values in PhraseList or not.</param>
		/// <returns>Result of setting phrase list.</returns>
		public async Task<SpeechRecognitionCompilationResult> SetPhraseListAsync(string commandSetName, string phraseListName, IEnumerable<string> phraseListValues, bool clearOldValues = true)
		{
			if (string.IsNullOrEmpty(this.m_filePath))
				throw new InvalidOperationException("Can not set phrase list because no file or voice commands were loaded. Please load voice commands first...");
			var commandSet = this.m_voiceCommands.CommandSets.Where(i => i.Name == commandSetName).FirstOrDefault();
			if (commandSet == null)
				throw new ArgumentException("Voice commands does not contains Command Set with name " + commandSetName + " !");
			SpeechRecognitionManager.SetPhraseList(m_voiceCommands, phraseListName, phraseListValues, clearOldValues);
			var grammar = SpeechRecognitionManager.CompileGrammar(commandSet);
			return await this.CompileConstraintAsync(grammar, commandSet.Name);
		}

		/// <summary>
		/// Sets the action to the specific commands.
		/// </summary>
		/// <param name="commandSet">CommandsSet name inside which is command located..</param>
		/// <param name="command">Command after which recognition is triggered a specific event.</param>
		/// <param name="callback">Action that will be performed</param>
		public void SetAction(string commandSet, string command, EventHandler<SpeechRecognitionResult> callback)
		{
			if (m_voiceCommands == null)
				throw new ArgumentException("No file or voice commands were set so no callback can be set...");

			var cmdSet = m_voiceCommands.CommandSets.Where(i => i.Name == commandSet).FirstOrDefault();
			if (cmdSet != null)
			{
				var cmd = cmdSet.Commands.Where(i => i.Name == command).FirstOrDefault();
				if (cmd != null)
				{
					cmd.VoiceAction.CommandRecognized += callback;
				}
				else
					throw new ArgumentException(commandSet + " does not contains commands with name '" + command + "'.");
			}
			else
				throw new ArgumentException("Voice commands does not contains commandSet with name '" + commandSet + "'.");
		}

		/// <summary>
		/// Deletes action for the specific command.
		/// </summary>
		/// <param name="commandSet">CommandsSet name inside which is command located..</param>
		/// <param name="command">Command after which recognition is triggered a specific event.</param>
		/// <param name="callback">Action that will be performed</param>
		public void DeleteAction(string commandSet, string command, EventHandler<SpeechRecognitionResult> callback)
		{
			if (m_voiceCommands == null)
				throw new ArgumentException("No file or voice commands were set so no callback can be set...");

			var cmdSet = m_voiceCommands.CommandSets.Where(i => i.Name == commandSet).FirstOrDefault();
			if (cmdSet != null)
			{
				var cmd = cmdSet.Commands.Where(i => i.Name == command).FirstOrDefault();
				if (cmd != null)
				{
					cmd.VoiceAction.CommandRecognized -= callback;
				}
				else
					throw new ArgumentException(commandSet + " does not contains commands with name '" + command + "'.");
			}
			else
				throw new ArgumentException("Voice commands does not contains commandSet with name '" + commandSet + "'.");
		}

		/// <summary>
		/// Stops current recognition asynchronously.
		/// </summary>
		public async Task StopRecognitionAsync()
		{
			await this.m_constrainedRecognizer.StopRecognitionAsync();
			await this.m_freeSpeechRecognizer.StopRecognitionAsync();
		}

		/// <summary>
		/// Global event. If is this event set it will be always called when any command will be recognized. The result
		/// and sender will be sent as parameter. Senders could be <see cref="CortanaIntegrator"/> or <see cref="RecognitionAndAction.SpeechNavigator"/>.
		/// </summary>
		public event EventHandler<SpeechRecognitionResult> CommandRecognized;

		public SpeechRecognizerTimeouts Timeouts
		{
			get { return m_constrainedRecognizer.Timeouts; }
		}
	}

    /// <summary>
    /// Indicates error in VCD commands compilation.
    /// </summary>
    public class GrammarCompilationException : Exception
    {
        public GrammarCompilationException() : base() { }
        public GrammarCompilationException(string message) : base(message) { }
        public GrammarCompilationException(string message, Exception inner) : base(message, inner) { }
    }
}
